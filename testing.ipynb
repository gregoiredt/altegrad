{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import read_graph, retrieve_subgraph\n",
    "\n",
    "\n",
    "device = 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "doc2vec_model= Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n",
      "Number of authors : 174961\n",
      "The minimum degree of the nodes in the graph is : 1\n",
      "The maximum degree of the nodes in the graph is : 3037\n",
      "The mean degree of the nodes in the graph is : 15.76841710048448\n",
      "The median degree of the nodes in the graph is : 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26498it [00:00, 263071.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in subgraph: 115041\n",
      "Number of edges in subgraph: 928816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115041it [00:00, 255764.53it/s]\n",
      "C:\\Users\\dutot\\Utilities\\anaconda3\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  return th.as_tensor(data, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=115041, num_edges=1857632,\n",
      "      ndata_schemes={'feat': Scheme(shape=(50,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "G, abstract, _, _ = read_graph()\n",
    "G = retrieve_subgraph(G, min_nb_nodes=3)\n",
    "attrs_n = []\n",
    "for i, node in tqdm(enumerate(G.nodes())):\n",
    "    G.nodes[int(node)]['feat'] = doc2vec_model.dv.get_vector(int(node))\n",
    "\n",
    "G = dgl.from_networkx(G, node_attrs=['feat']) # already undirected\n",
    "print(G)\n",
    "node_features = G.ndata['feat']\n",
    "num_features = node_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "negative_sampler = dgl.dataloading.negative_sampler.Uniform(5)\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([5, 10])\n",
    "train_dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "    # The following arguments are specific to NodeDataLoader.\n",
    "    G,                                  # The graph\n",
    "    torch.arange(G.number_of_edges()),  # The edges to iterate over\n",
    "    sampler,                                # The neighbor sampler\n",
    "    negative_sampler=negative_sampler,      # The negative sampler\n",
    "    device=device,                          # Put the MFGs on CPU or GPU\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=1024,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=False,    # Whether to drop the last incomplete batch\n",
    "    num_workers=0       # Number of sampler processes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input nodes: 72174\n",
      "Positive graph # nodes: 6887 # edges: 1024\n",
      "Negative graph # nodes: 6887 # edges: 5120\n",
      "[Block(num_src_nodes=72174, num_dst_nodes=36925, num_edges=175776), Block(num_src_nodes=36925, num_dst_nodes=6887, num_edges=53134)]\n"
     ]
    }
   ],
   "source": [
    "input_nodes, pos_graph, neg_graph, mfgs = next(iter(train_dataloader))\n",
    "print('Number of input nodes:', len(input_nodes))\n",
    "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
    "print('Negative graph # nodes:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\n",
    "print(mfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_models import SageModel, inference, DotPredictor\n",
    "model = SageModel(node_features.shape[1], 128).to(device)\n",
    "predictor = DotPredictor().to(device)\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a97ca9059479>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmfgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m# feature copy from CPU to GPU takes place here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(1):\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, pos_graph, neg_graph, mfgs) in enumerate(tq):\n",
    "            # feature copy from CPU to GPU takes place here\n",
    "            inputs = mfgs[0].srcdata['feat']\n",
    "\n",
    "            outputs = model(mfgs, inputs)\n",
    "            pos_score = predictor(pos_graph, outputs)\n",
    "            neg_score = predictor(neg_graph, outputs)\n",
    "\n",
    "            score = torch.cat([pos_score, neg_score])\n",
    "            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "\n",
    "            if (step + 1) % 500 == 0:\n",
    "\n",
    "\n",
    "                # Note that this tutorial do not train the whole model to the end.\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and getting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "best_model_path = 'model.pt'\n",
    "model = SageModel(node_features.shape[1], 128).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "embeddings = model.conv1(G, node_features.to(device)) # Confianc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_hidden, n_input) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_input = n_input\n",
    "        self.f1 = nn.Linear(n_input, n_hidden)\n",
    "        self.f2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.f3 = nn.Linear(n_hidden, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.f1(x))\n",
    "        x = self.dropout(self.relu(self.f2(x)))\n",
    "        output = self.sigmoid(self.f3(x))\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1857632/1857632 [00:08<00:00, 209305.30it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = G.nodes()\n",
    "edges = G.edges()\n",
    "ledges = []\n",
    "for i in tqdm(range(len(edges[0]))):\n",
    "    ledges.append((edges[0][i].item(), edges[1][i].item()))\n",
    "np.random.shuffle(ledges)\n",
    "train_edges = ledges[:round(0.6*len(ledges))]\n",
    "test_edges = ledges[round(0.6*len(ledges)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_edge \n",
    "device = torch.device('cuda')\n",
    "batch_size = 1000\n",
    "epochs = 500\n",
    "mlp = MLP(n_hidden=200, n_input=256).to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1114 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    with tqdm(range(len(train_edges)//batch_size)) as tq:\n",
    "        for n_batch in tq:\n",
    "            batch = train_edges[n_batch*batch_size:(n_batch+1)*batch_size]\n",
    "            x = torch.FloatTensor(size=(batch_size,256))\n",
    "            x_neg = torch.FloatTensor(size=(batch_size,256))\n",
    "            for n, (i,j) in enumerate(batch):\n",
    "                feature = torch.cat([embeddings[i], embeddings[j]])\n",
    "                a, b= random_edge(len(nodes))\n",
    "                feature_neg = torch.cat([embeddings[a], embeddings[b]])\n",
    "                x[n] = feature\n",
    "                x_neg[n] = feature_neg\n",
    "            x = torch.cat([x, x_neg])\n",
    "            y = mlp(x)\n",
    "            target = torch.ones(batch_size)\n",
    "            target = torch.cat([target, torch.zeros(batch_size)])\n",
    "            loss = criterion(y.squeeze(), target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f'Epoch : {epoch} Mean Loss : {np.mean(losses)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from preprocessing import read_graph\n",
    "from time import time \n",
    "from nltk.tokenize import word_tokenize\n",
    "_, abstracts, _, _ = read_graph()\n",
    "# ~4 mins\n",
    "data = list(abstract.values())\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "vec_size = 50\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=2,\n",
    "                hs=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    t_start = time()\n",
    "    \n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=1)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    print('Epoch {0}. Elapsed {1} s'.format(epoch, time() - t_start))\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
